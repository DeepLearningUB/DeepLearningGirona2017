{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `keras`\n",
    "\n",
    "> Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. \n",
    "\n",
    "```Python\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking layers is as easy as ``.add()``:\n",
    "\n",
    "```Python\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model looks good, configure its learning process with ``.compile()``:\n",
    "\n",
    "```Python\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer.\n",
    "\n",
    "```Python\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now iterate on your training data in batches:\n",
    "\n",
    "```Python\n",
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
    "```\n",
    "\n",
    "Evaluate your performance in one line:\n",
    "```Python\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or generate predictions on new data:\n",
    "\n",
    "```Python\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST  MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\n",
      "15269888/15296311 [============================>.] - ETA: 0s60000 train samples\n",
      "10000 test samples\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 512)           401920      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 512)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           262656      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 512)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10)            0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.2442 - acc: 0.9237 - val_loss: 0.1226 - val_acc: 0.9618\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.1030 - acc: 0.9689 - val_loss: 0.0806 - val_acc: 0.9743\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 12s - loss: 0.0757 - acc: 0.9778 - val_loss: 0.0765 - val_acc: 0.9771\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s - loss: 0.0600 - acc: 0.9825 - val_loss: 0.0848 - val_acc: 0.9764\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 12s - loss: 0.0506 - acc: 0.9847 - val_loss: 0.0849 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.0435 - acc: 0.9867 - val_loss: 0.0847 - val_acc: 0.9791\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.0399 - acc: 0.9877 - val_loss: 0.0739 - val_acc: 0.9824\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.0356 - acc: 0.9895 - val_loss: 0.0884 - val_acc: 0.9805\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.0314 - acc: 0.9909 - val_loss: 0.0995 - val_acc: 0.9784\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 13s - loss: 0.0283 - acc: 0.9923 - val_loss: 0.0951 - val_acc: 0.9808\n",
      "\n",
      "\n",
      "Test score: 0.0951370499239\n",
      "Test accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Trains a simple deep NN on the MNIST dataset.\n",
    "You can get to 98.40% test accuracy after 20 epochs.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test  = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# print model characteristics\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=batch_size, \n",
    "                    nb_epoch=nb_epoch,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``keras`` sequential mode\n",
    "\n",
    "The ``Sequential`` model is a linear stack of layers.\n",
    "\n",
    "You can create a ``Sequential`` model by passing a list of layer instances to the constructor:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim=784),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "```\n",
    "\n",
    "You can also simply add layers via the ``.add()`` method:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``merge`` layer\n",
    "\n",
    "Multiple Sequential instances can be merged into a single output via a ``Merge`` layer. The output is a layer that can be added as first layer in a new ``Sequential`` model. For instance, here's a model with two separate input branches getting merged:\n",
    "\n",
    "```python\n",
    "from keras.layers import Merge\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "![alt text](images/merge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a two-branch model can then be trained via e.g.:\n",
    "\n",
    "```python\n",
    "final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "final_model.fit([input_data_1, input_data_2], targets)  # we pass one data array per model input\n",
    "```\n",
    "\n",
    "The ``Merge`` layer supports a number of pre-defined modes:\n",
    "\n",
    "+ ``sum`` (default): element-wise sum\n",
    "+ ``concat``: tensor concatenation. You can specify the concatenation axis via the argument concat_axis.\n",
    "+ ``mul``: element-wise multiplication\n",
    "+ ``ave``: tensor average\n",
    "+ ``dot``: dot product. You can specify which axes to reduce along via the argument dot_axes.\n",
    "+ ``cos``: cosine proximity between vectors in 2D tensors.\n",
    "\n",
    "You can also pass a function as the mode argument, allowing for arbitrary transformations:\n",
    "\n",
    "```python\n",
    "merged = Merge([left_branch, right_branch], mode=lambda x: x[0] - x[1])\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "+ an optimizer. This could be the string identifier of an existing optimizer (such as ``rmsprop`` or ``adagrad``), or an instance of the  ``Optimizer`` class. \n",
    "+ a loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as ``categorical_crossentropy`` or ``mse``), or it can be an objective function. \n",
    "+ a list of metrics. For any classification problem you will want to set this to ``metrics=['accuracy']``. A metric could be the string identifier of an existing metric or a custom metric function. Custom metric function should return either a single tensor value or a dict ``metric_name`` -> ``metric_value``. \n",
    "\n",
    "```python\n",
    "# for a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# for a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# for a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "``Keras`` models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  ``fit`` function.\n",
    "\n",
    "For a single-input model with 2 classes (binary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7311 - acc: 0.4910     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7166 - acc: 0.5130     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7049 - acc: 0.5300     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7005 - acc: 0.5320     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6826 - acc: 0.5730     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6861 - acc: 0.5590     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6725 - acc: 0.5890     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6644 - acc: 0.5980     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6609 - acc: 0.6120     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6516 - acc: 0.6210     \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_4 (Dense)                  (None, 1)             785         dense_input_2[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 785\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=784, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 784))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "\n",
    "# train the model, iterating on the data in batches\n",
    "# of 32 samples\n",
    "model.fit(data, labels, nb_epoch=10, batch_size=32)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multi-input model with 10 classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 3.0415 - acc: 0.0930     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.6647 - acc: 0.1190     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.4765 - acc: 0.1540     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.4084 - acc: 0.1700     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2640 - acc: 0.1890     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.1660 - acc: 0.2230     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.0763 - acc: 0.2560     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.0243 - acc: 0.2670     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.8915 - acc: 0.3350     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.8228 - acc: 0.3700     \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_5 (Dense)                  (None, 32)            25120                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 32)            25120                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 10)            650         merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 50890\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(merged)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "data_1 = np.random.random((1000, 784))\n",
    "data_2 = np.random.random((1000, 784))\n",
    "\n",
    "# these are integers between 0 and 9\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "# we convert the labels to a binary matrix of size (1000, 10)\n",
    "# for use with categorical_crossentropy\n",
    "labels = to_categorical(labels, 10)\n",
    "\n",
    "# train the model\n",
    "# note that we are passing a list of Numpy arrays as training data\n",
    "# since the model has 2 inputs\n",
    "model.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras functional API\n",
    "\n",
    "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "The ``Sequential`` model is probably a better choice to implement such a network, but it helps to start with something really simple.\n",
    "\n",
    "Using ``Model`` class:\n",
    "\n",
    "+ A layer instance is callable (on a tensor), and it returns a tensor\n",
    "+ ``Input`` tensor(s) and output tensor(s) can then be used to define a ``Model``\n",
    "+ Such a model can be trained just like Keras Sequential models.\n",
    "\n",
    "```python\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.fit(data, labels)  # starts training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are callable, just like layers.\n",
    "\n",
    "With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the architecture of the model, you are also re-using its weights.\n",
    "\n",
    "```python\n",
    "x = Input(shape=(784,))\n",
    "# this works, and returns the 10-way softmax we defined above.\n",
    "y = model(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Siamese MLP on pairs of digits from the MNIST \n",
    "\n",
    "(Source: https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py)\n",
    "\n",
    "Siamese networks are commonly used in image comparison applications such as face or signature verification. They can also be used in language processing, times series analysis, etc.\n",
    "\n",
    "In a typical Siamese network a large part of the network is duplicated at the base to allow multiple inputs to go through identical layers. \n",
    "\n",
    "This example shows how to teach a neural network to map an image from the MNIST dataset to a 2D point, while trying to minimize the distance between points of the same class and maximize the distance between points of different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese network architecture is a way of learning how to embed samples into lower-dimensions based on similarity computed with features learned by a feature network.\n",
    "\n",
    "The feature network is the architecture we intend to fine-tune in this setting. \n",
    "\n",
    "Let's suppose we want to embed images. Given two images $X_1$ and $X_2$, we feed into the feature network $G_W$ and compute corresponding feature vectors $G_W(X_1)$ and $G_W(X_2)$. The final layer computes pair-wise distance between computed features $E_W = || G_W(X_1) - G_W(X_2) ||_{1}$ and final loss layer $L$ considers whether these two images are from the same class (label $1$) or not (label $0$).\n",
    "\n",
    "![alt text](images/siamese1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original [paper](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf) it was proposed the **Contrastive Loss Function**: \n",
    "\n",
    "$$ L(W,(Y,X_1,X_2)^i) = (1 - Y) \\times L_S(E_W(X_1,X_2)^i) + Y \\times L_D(E_W(X_1,X_2)^i) $$\n",
    "\n",
    "where $L_S$ is the partial loss function for a \"same-class\" pair and $L_D$ is the partial loss function for a \"different-class\" pair.\n",
    "\n",
    "$L_S$ and $L_D$ should be designed in such a way that the minimization of $L$ will decrease the distance in the embedding space of \"same-class\" pairs and increase it in the case of \"different-class\" pairs:\n",
    "\n",
    "$$ L_S = \\frac{1}{2} E_W^2 $$\n",
    "$$ L_D = \\frac{1}{2} \\{ \\mbox{max }(0,1-E_W) \\}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108400 samples, validate on 17820 samples\n",
      "Epoch 1/10\n",
      "108400/108400 [==============================] - 8s - loss: 0.0701 - val_loss: 0.0410\n",
      "Epoch 2/10\n",
      "108400/108400 [==============================] - 7s - loss: 0.0282 - val_loss: 0.0277\n",
      "Epoch 3/10\n",
      "108400/108400 [==============================] - 7s - loss: 0.0183 - val_loss: 0.0232\n",
      "Epoch 4/10\n",
      "108400/108400 [==============================] - 7s - loss: 0.0132 - val_loss: 0.0215\n",
      "Epoch 5/10\n",
      "108400/108400 [==============================] - 6s - loss: 0.0103 - val_loss: 0.0206\n",
      "Epoch 6/10\n",
      "108400/108400 [==============================] - 7s - loss: 0.0081 - val_loss: 0.0210\n",
      "Epoch 7/10\n",
      "108400/108400 [==============================] - 6s - loss: 0.0069 - val_loss: 0.0209\n",
      "Epoch 8/10\n",
      "108400/108400 [==============================] - 8s - loss: 0.0058 - val_loss: 0.0214\n",
      "Epoch 9/10\n",
      "108400/108400 [==============================] - 7s - loss: 0.0049 - val_loss: 0.0219\n",
      "Epoch 10/10\n",
      "108400/108400 [==============================] - 6s - loss: 0.0044 - val_loss: 0.0215\n",
      "* Accuracy on training set: 99.99%\n",
      "* Accuracy on test set: 99.61%\n"
     ]
    }
   ],
   "source": [
    "'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n",
    "It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n",
    "output of the shared network and by optimizing the contrastive loss (see paper\n",
    "for mode details).\n",
    "[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "Gets to 99.5% test accuracy after 20 epochs.\n",
    "3 seconds per epoch on a Titan X GPU\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n",
    "#    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "#    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "input_dim = 784\n",
    "nb_epoch = 10\n",
    "\n",
    "# create training+test positive and negative pairs\n",
    "digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n",
    "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
    "\n",
    "digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
    "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network(input_dim)\n",
    "\n",
    "input_a = Input(shape=(input_dim,))\n",
    "input_b = Input(shape=(input_dim,))\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model(input=[input_a, input_b], output=distance)\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer=rms)\n",
    "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
    "          batch_size=128,\n",
    "          nb_epoch=nb_epoch)\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(pred, tr_y)\n",
    "pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(pred, te_y)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/siameseresult.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
